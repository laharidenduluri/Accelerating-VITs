{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3932654,"sourceType":"datasetVersion","datasetId":2334639}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install thop\n!pip install timm","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:05:43.037880Z","iopub.execute_input":"2024-04-18T17:05:43.038687Z","iopub.status.idle":"2024-04-18T17:06:07.676661Z","shell.execute_reply.started":"2024-04-18T17:05:43.038650Z","shell.execute_reply":"2024-04-18T17:06:07.675679Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: thop in /opt/conda/lib/python3.10/site-packages (0.1.1.post2209072238)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.22.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, RandomHorizontalFlip, ToTensor, Normalize\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\nfrom timm import create_model\nfrom torch import nn, autograd, max, sum\nimport torch\nfrom torchvision import transforms\nfrom thop import profile\nimport os\n\n# Model and pruning configuration (adjust as needed)\nmodel_name = \"swin_tiny_patch4_window7_224\"  # Choose the desired Swin Transformer model\npruning_ratio = 0.45  # Start with lower pruning ratios\nnum_training_epochs = 10 # Train longer before pruning\nnum_finetuning_epochs = 2  # Fine-tune longer to recover\nlearning_rate = 0.002  # Slightly lower learning rate\nlearning_rate_finetune = 0.001  # Fine-tuning learning rate\nthreshold = 0.3  \n\n# Load dataset and apply appropriate transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset_root = \"/kaggle/input/caltech-101\"\n\n# Load the entire dataset\nfull_dataset = ImageFolder(root=dataset_root, transform=transform)\n\n# Split the dataset into train and test subsets (e.g., 80% train, 20% test)\ntrain_size = int(0.8 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\ndef get_original_model_size(model: nn.Module, model_path: str = \"/tmp/model.pt\") -> int:\n    \"\"\"\n    Gets the size of the PyTorch model in bytes.\n\n    Args:\n        model (nn.Module): The PyTorch model.\n        model_path (str, optional): The path to save the model. Defaults to \"/tmp/model.pt\".\n\n    Returns:\n        int: The size of the model in bytes.\n    \"\"\"\n    torch.save(model.state_dict(), model_path)\n    return os.path.getsize(model_path)\n\n\ndef get_pruned_model_size(model: nn.Module, model_path: str = \"/tmp/model.pt\") -> int:\n    \"\"\"\n    Gets the size of the PyTorch model in bytes, considering pruned weights.\n\n    Args:\n        model (nn.Module): The PyTorch model.\n        model_path (str, optional): The path to save the model. Defaults to \"/tmp/model.pt\".\n\n    Returns:\n        int: The size of the model in bytes.\n    \"\"\"\n    # Save the pruned model state\n    torch.save(model.state_dict(), model_path)\n    num_elements = 0\n    # Iterate over values in the state dict (weights and biases)\n    for param in model.state_dict().values():\n        num_elements += param.numel()\n    # Calculate size assuming 4 bytes per element (common for floats)\n    return num_elements * 4\n\n\ndef count_model_parameters(model: nn.Module) -> int:\n    \"\"\"\n    Counts the total number of non-zero parameters in a PyTorch model.\n\n    Args:\n        model (nn.Module): The PyTorch model to count the non-zero parameters of.\n\n    Returns:\n        int: The total number of non-zero parameters in the model.\n    \"\"\"\n\n    total_non_zero_params = 0\n    for param in model.parameters():\n        # Count non-zero elements using torch.count_nonzero for efficiency\n        total_non_zero_params += torch.count_nonzero(param).item()\n\n    return total_non_zero_params\n\n\ndef count_flops(model: nn.Module) -> int:\n  \"\"\"\n  Counts the approximate number of floating-point operations (FLOPs) for a PyTorch model,\n  considering the sparsity of weights due to pruning.\n\n  Args:\n    model (nn.Module): The PyTorch model.\n\n  Returns:\n    int: The estimated number of FLOPs.\n  \"\"\"\n  total_flops = 0\n  for name, module in model.named_modules():\n    if isinstance(module, (nn.Conv2d, nn.Linear)):\n      if isinstance(module, nn.Conv2d):\n        in_features = module.in_channels\n        out_features = module.out_channels\n        kernel_size = module.kernel_size\n      else:\n        in_features = module.in_features\n        out_features = module.out_features\n        kernel_size = (1, 1)  # Kernel size is 1 for Linear layers\n\n      # Consider sparsity by multiplying with the number of non-zero elements in weights\n      num_non_zero_weights = torch.count_nonzero(module.weight).item()\n      flops_per_output = in_features * out_features * kernel_size[0] * kernel_size[1]\n      total_flops += num_non_zero_weights * flops_per_output\n\n  return total_flops\n\n\ndef prune_heads(model, pruning_ratio):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.MultiheadAttention):\n            print(\"Processing attention layer:\", name)\n            num_heads = module.num_heads\n            head_importance_scores = torch.norm(module.in_proj_weight, dim=0)  # Calculate L2 norm for each head\n            num_heads_to_prune = int(num_heads * pruning_ratio)\n            _, sorted_head_indices = torch.topk(head_importance_scores, num_heads_to_prune)\n\n            # Prune the least important heads\n            module.num_heads -= num_heads_to_prune\n            module.in_proj_weight = nn.Parameter(module.in_proj_weight[:, sorted_head_indices])\n            module.out_proj.weight = nn.Parameter(module.out_proj.weight[sorted_head_indices])\n\n            print(f\"Pruned {num_heads_to_prune} heads. Remaining heads: {module.num_heads}\")\n\n\n\ndef prune_model_with_svd_and_iht(model, pruning_ratio, num_iterations=10):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            print(\"Processing layer:\", name)\n            weight = module.weight.data\n            # Apply SVD\n            print(\"Applying SVD...\")\n            u, s, v = torch.svd(weight)\n            s_pruned = torch.zeros_like(s)\n            s_pruned[:int(s.size(0) * (1 - pruning_ratio))] = s[:int(s.size(0) * (1 - pruning_ratio))]\n            weight_pruned_svd = torch.mm(u, torch.mm(torch.diag(s_pruned), v.t()))\n            num_params_after_svd = count_model_parameters(model)\n            print(\"Number of parameters after SVD pruning:\", count_model_parameters(model))\n\n            print(f\"Number of parameters after svd pruning: {num_params_after_svd}\")\n          # Apply IHT with masking for better pruning\n            print(\"Applying IHT...\")\n            weight_pruned_iht = iht(weights=weight_pruned_svd, pruning_ratio=pruning_ratio,\n                                    num_iterations=num_iterations, threshold=threshold, module=module)\n            \n            # Set the pruned weights and clear gradients (important for training)\n            module.weight.data = weight_pruned_iht\n            module.weight.grad = None\n           \n\ndef iht(weights, pruning_ratio, num_iterations, threshold, module):\n    for _ in range(num_iterations):\n        # Calculate gradients using torch.autograd.grad with retain_graph=True to avoid recomputing previous gradients\n        gradients = torch.autograd.grad(module.weight, module.weight, grad_outputs=weights, retain_graph=True)[0]\n\n        # Update weights using calculated gradients, masking to avoid overwriting pruned values\n        mask = torch.abs(weights) >= threshold\n        weights[mask] = torch.clamp(weights[mask] - learning_rate * gradients[mask], -threshold, threshold)\n\n    # Apply hard thresholding and clear gradients\n    weights[torch.abs(weights) < threshold] = 0\n    weights.grad = None\n    num_params_after_svdandiht = count_model_parameters(model)\n    print(f\"Number of parameters after svdandiht pruning: {num_params_after_svdandiht}\")\n\n    return weights\n\n\n# Load the model once before the loop\nmodel = create_model(model_name, pretrained=True)\n\ndef optimizer_creator():\n    return torch.optim.SGD(model.parameters(), lr=learning_rate)  # Create optimizer within a closure\n# Load the model once before the loop\nmodel = create_model(model_name, pretrained=True)\ninput_size = (16, 3, 224, 224) \n\n\n\noptimizer = optimizer_creator()\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(num_training_epochs):\n    train_loss = 0.0\n    train_acc = 0.0\n    for images, labels in train_loader:\n        optimizer.zero_grad()  # Clear gradients for the current step\n        outputs = model(images)\n        logits = outputs  # Access logits for Swin Transformers\n        loss = criterion(logits, labels)\n        loss.backward()  # Backpropagate gradients\n        optimizer.step()  # Update model parameters based on gradients\n        train_loss += loss.item()  # Accumulate training loss\n\n        \n        # Calculate training accuracy\n        _, preds = torch.max(outputs, 1)\n        train_acc += torch.sum(preds == labels).item() / len(labels)\n\n    # Print training progress\n    print(f\"Epoch [{epoch+1}/{num_training_epochs}], Pruning Ratio: {pruning_ratio:.2f}, \"\n            f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc/len(train_loader):.4f}\")\n\n\nflops_before_pruning = count_flops(model)\nprint(f\"Estimated FLOPs before pruning: {flops_before_pruning:.2e}\")\n\n# Print the model size before pruning\nprint(f\"Model size before pruning: {get_original_model_size(model)} bytes\")\n\nnum_params_before = count_model_parameters(model)\nprint(f\"Number of parameters before pruning: {num_params_before}\")\n\n# Count the FLOPs before pruning\nflops_before, _ = profile(model, inputs=(torch.randn(1, 3, 224, 224), ))\nprint(f\"Number of FLOPs before pruning: {flops_before:.2f} GFLOPs\")\nprune_heads(model, pruning_ratio=pruning_ratio)\n\n# Apply pruning with the current pruning ratio\nprune_model_with_svd_and_iht(model, pruning_ratio=pruning_ratio)\n\n# Calculate FLOPs after pruning\nflops_after_pruning = count_flops(model)\nprint(f\"Estimated FLOPs after pruning: {flops_after_pruning:.2e}\")\n\n# Calculate model size after pruning considering sparsity\npruned_model_size_sparse = get_pruned_model_size(model)\nprint(f\"Model size after pruning (considering sparsity): {pruned_model_size_sparse} bytes\")\n\n# **Optional:** Save the entire state dictionary after pruning (original functionality)\npruned_model_size_all = get_original_model_size(model)  # Assuming you have this functionality\nprint(f\"Model size after pruning (entire state dict): {pruned_model_size_all} bytes\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:06:55.826727Z","iopub.execute_input":"2024-04-18T17:06:55.827119Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Estimated FLOPs before pruning: 3.69e+13\nModel size before pruning: 113207542 bytes\nNumber of parameters before pruning: 28288354\n[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\nNumber of FLOPs before pruning: 4371851808.00 GFLOPs\nProcessing layer: layers.0.blocks.0.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 28288354\nNumber of parameters after svd pruning: 28288354\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28288354\nProcessing layer: layers.0.blocks.0.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 28260706\nNumber of parameters after svd pruning: 28260706\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28260706\nProcessing layer: layers.0.blocks.0.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 28251490\nNumber of parameters after svd pruning: 28251490\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28251490\nProcessing layer: layers.0.blocks.0.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 28214626\nNumber of parameters after svd pruning: 28214626\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28214626\nProcessing layer: layers.0.blocks.1.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 28177762\nNumber of parameters after svd pruning: 28177762\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28177762\nProcessing layer: layers.0.blocks.1.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 28150114\nNumber of parameters after svd pruning: 28150114\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28150114\nProcessing layer: layers.0.blocks.1.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 28140898\nNumber of parameters after svd pruning: 28140898\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28140898\nProcessing layer: layers.0.blocks.1.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 28104034\nNumber of parameters after svd pruning: 28104034\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28104034\nProcessing layer: layers.1.downsample.reduction\nApplying SVD...\nNumber of parameters after SVD pruning: 28067170\nNumber of parameters after svd pruning: 28067170\nApplying IHT...\nNumber of parameters after svdandiht pruning: 28067170\nProcessing layer: layers.1.blocks.0.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 27993442\nNumber of parameters after svd pruning: 27993442\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27993442\nProcessing layer: layers.1.blocks.0.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 27882850\nNumber of parameters after svd pruning: 27882850\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27882850\nProcessing layer: layers.1.blocks.0.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 27845986\nNumber of parameters after svd pruning: 27845986\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27845986\nProcessing layer: layers.1.blocks.0.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 27698530\nNumber of parameters after svd pruning: 27698530\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27698530\nProcessing layer: layers.1.blocks.1.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 27551074\nNumber of parameters after svd pruning: 27551074\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27551074\nProcessing layer: layers.1.blocks.1.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 27440482\nNumber of parameters after svd pruning: 27440482\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27440482\nProcessing layer: layers.1.blocks.1.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 27403618\nNumber of parameters after svd pruning: 27403618\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27403618\nProcessing layer: layers.1.blocks.1.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 27256162\nNumber of parameters after svd pruning: 27256162\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27256162\nProcessing layer: layers.2.downsample.reduction\nApplying SVD...\nNumber of parameters after SVD pruning: 27108706\nNumber of parameters after svd pruning: 27108706\nApplying IHT...\nNumber of parameters after svdandiht pruning: 27108706\nProcessing layer: layers.2.blocks.0.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 26813794\nNumber of parameters after svd pruning: 26813794\nApplying IHT...\nNumber of parameters after svdandiht pruning: 26813794\nProcessing layer: layers.2.blocks.0.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 26371426\nNumber of parameters after svd pruning: 26371426\nApplying IHT...\nNumber of parameters after svdandiht pruning: 26371426\nProcessing layer: layers.2.blocks.0.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 26223970\nNumber of parameters after svd pruning: 26223970\nApplying IHT...\nNumber of parameters after svdandiht pruning: 26223970\nProcessing layer: layers.2.blocks.0.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 25634146\nNumber of parameters after svd pruning: 25634146\nApplying IHT...\nNumber of parameters after svdandiht pruning: 25634146\nProcessing layer: layers.2.blocks.1.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 25044322\nNumber of parameters after svd pruning: 25044322\nApplying IHT...\nNumber of parameters after svdandiht pruning: 25044322\nProcessing layer: layers.2.blocks.1.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 24601954\nNumber of parameters after svd pruning: 24601954\nApplying IHT...\nNumber of parameters after svdandiht pruning: 24601954\nProcessing layer: layers.2.blocks.1.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 24454498\nNumber of parameters after svd pruning: 24454498\nApplying IHT...\nNumber of parameters after svdandiht pruning: 24454498\nProcessing layer: layers.2.blocks.1.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 23864674\nNumber of parameters after svd pruning: 23864674\nApplying IHT...\nNumber of parameters after svdandiht pruning: 23864674\nProcessing layer: layers.2.blocks.2.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 23274850\nNumber of parameters after svd pruning: 23274850\nApplying IHT...\nNumber of parameters after svdandiht pruning: 23274850\nProcessing layer: layers.2.blocks.2.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 22832482\nNumber of parameters after svd pruning: 22832482\nApplying IHT...\nNumber of parameters after svdandiht pruning: 22832482\nProcessing layer: layers.2.blocks.2.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 22685026\nNumber of parameters after svd pruning: 22685026\nApplying IHT...\nNumber of parameters after svdandiht pruning: 22685026\nProcessing layer: layers.2.blocks.2.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 22095202\nNumber of parameters after svd pruning: 22095202\nApplying IHT...\nNumber of parameters after svdandiht pruning: 22095202\nProcessing layer: layers.2.blocks.3.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 21505378\nNumber of parameters after svd pruning: 21505378\nApplying IHT...\nNumber of parameters after svdandiht pruning: 21505378\nProcessing layer: layers.2.blocks.3.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 21063010\nNumber of parameters after svd pruning: 21063010\nApplying IHT...\nNumber of parameters after svdandiht pruning: 21063010\nProcessing layer: layers.2.blocks.3.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 20915554\nNumber of parameters after svd pruning: 20915554\nApplying IHT...\nNumber of parameters after svdandiht pruning: 20915554\nProcessing layer: layers.2.blocks.3.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 20325730\nNumber of parameters after svd pruning: 20325730\nApplying IHT...\nNumber of parameters after svdandiht pruning: 20325730\nProcessing layer: layers.2.blocks.4.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 19735906\nNumber of parameters after svd pruning: 19735906\nApplying IHT...\nNumber of parameters after svdandiht pruning: 19735906\nProcessing layer: layers.2.blocks.4.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 19293538\nNumber of parameters after svd pruning: 19293538\nApplying IHT...\nNumber of parameters after svdandiht pruning: 19293538\nProcessing layer: layers.2.blocks.4.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 19146082\nNumber of parameters after svd pruning: 19146082\nApplying IHT...\nNumber of parameters after svdandiht pruning: 19146082\nProcessing layer: layers.2.blocks.4.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 18556258\nNumber of parameters after svd pruning: 18556258\nApplying IHT...\nNumber of parameters after svdandiht pruning: 18556258\nProcessing layer: layers.2.blocks.5.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 17966434\nNumber of parameters after svd pruning: 17966434\nApplying IHT...\nNumber of parameters after svdandiht pruning: 17966434\nProcessing layer: layers.2.blocks.5.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 17524066\nNumber of parameters after svd pruning: 17524066\nApplying IHT...\nNumber of parameters after svdandiht pruning: 17524066\nProcessing layer: layers.2.blocks.5.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 17376610\nNumber of parameters after svd pruning: 17376610\nApplying IHT...\nNumber of parameters after svdandiht pruning: 17376610\nProcessing layer: layers.2.blocks.5.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 16786786\nNumber of parameters after svd pruning: 16786786\nApplying IHT...\nNumber of parameters after svdandiht pruning: 16786786\nProcessing layer: layers.3.downsample.reduction\nApplying SVD...\nNumber of parameters after SVD pruning: 16196962\nNumber of parameters after svd pruning: 16196962\nApplying IHT...\nNumber of parameters after svdandiht pruning: 16196962\nProcessing layer: layers.3.blocks.0.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 15017314\nNumber of parameters after svd pruning: 15017314\nApplying IHT...\nNumber of parameters after svdandiht pruning: 15017314\nProcessing layer: layers.3.blocks.0.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 13247842\nNumber of parameters after svd pruning: 13247842\nApplying IHT...\nNumber of parameters after svdandiht pruning: 13247842\nProcessing layer: layers.3.blocks.0.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 12658018\nNumber of parameters after svd pruning: 12658018\nApplying IHT...\nNumber of parameters after svdandiht pruning: 12658018\nProcessing layer: layers.3.blocks.0.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 10298722\nNumber of parameters after svd pruning: 10298722\nApplying IHT...\nNumber of parameters after svdandiht pruning: 10298722\nProcessing layer: layers.3.blocks.1.attn.qkv\nApplying SVD...\nNumber of parameters after SVD pruning: 7939426\nNumber of parameters after svd pruning: 7939426\nApplying IHT...\nNumber of parameters after svdandiht pruning: 7939426\nProcessing layer: layers.3.blocks.1.attn.proj\nApplying SVD...\nNumber of parameters after SVD pruning: 6169954\nNumber of parameters after svd pruning: 6169954\nApplying IHT...\nNumber of parameters after svdandiht pruning: 6169954\nProcessing layer: layers.3.blocks.1.mlp.fc1\nApplying SVD...\nNumber of parameters after SVD pruning: 5580130\nNumber of parameters after svd pruning: 5580130\nApplying IHT...\nNumber of parameters after svdandiht pruning: 5580130\nProcessing layer: layers.3.blocks.1.mlp.fc2\nApplying SVD...\nNumber of parameters after SVD pruning: 3220834\nNumber of parameters after svd pruning: 3220834\nApplying IHT...\nNumber of parameters after svdandiht pruning: 3220834\nProcessing layer: head.fc\nApplying SVD...\nNumber of parameters after SVD pruning: 861538\nNumber of parameters after svd pruning: 861538\nApplying IHT...\nNumber of parameters after svdandiht pruning: 861538\nEstimated FLOPs after pruning: 2.12e+07\nModel size after pruning (considering sparsity): 113154208 bytes\nModel size after pruning (entire state dict): 113267678 bytes\n","output_type":"stream"}]}]}